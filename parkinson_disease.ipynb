{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkzlSv3imth7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "r7zjM-Rrrczn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "02e2e841-0929-45fc-b116-335e3ac841ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/archive/parkinsons.data')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CIx2acxdmzzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to take inputs for machine learning\n",
        "features = df.loc[:, df.columns != 'status'].values[:, 1:]\n",
        "labels = df.loc[:, 'status'].values"
      ],
      "metadata": {
        "id": "f9bwnYGcm-rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use for classification the values\n",
        "labels[labels == 1].shape[0], labels[labels == 0].shape[0]\n"
      ],
      "metadata": {
        "id": "nKmweKeRnB7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "# indicate that a feature or function is deprecated and will be removed in future versions of the library\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler((-1, 1))\n",
        "X = scaler.fit_transform(features)\n",
        "Y = labels"
      ],
      "metadata": {
        "id": "SBlvVsUFnGFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.14, random_state=7)"
      ],
      "metadata": {
        "id": "s9EAxf7vnI2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "#  to make predictions based on the relationship between the features and labels.\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "sSRjSrJInLY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "udf = pd.read_csv('/content/drive/MyDrive/archive/parkinsons.data')\n",
        "udf.head()"
      ],
      "metadata": {
        "id": "G9vJOaVu9YXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = udf.loc[:, (udf.columns != 'motor_UPDRS') & (udf.columns != 'total_UPDRS')].values[:, 1:]\n",
        "labels = udf.loc[:, (udf.columns == 'motor_UPDRS') | (udf.columns == 'total_UPDRS')].values"
      ],
      "metadata": {
        "id": "dluDEuNgnSJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=7)\n"
      ],
      "metadata": {
        "id": "cNfc7-ZAndWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM"
      ],
      "metadata": {
        "id": "9LEia94enjqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u_model = Sequential()\n",
        "u_model.add(Dense(32, input_shape=(X.shape[1],)))\n",
        "u_model.add(Dense(16, activation='tanh'))\n",
        "u_model.add(Dense(8, activation='tanh'))\n",
        "u_model.add(Dense(72, activation='tanh'))\n",
        "u_model.add(Dense(Y.shape[0], activation='tanh'))\n",
        "# Stochastic Gradient Descent ('sgd')\n",
        "u_model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "u_model.fit(X_train, Y_train, batch_size=1, epochs=5, validation_split=0.25, shuffle=True)"
      ],
      "metadata": {
        "id": "XbZLVIA4nmcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u_model.fit(X_train, Y_train, batch_size=1, epochs=15, validation_split=0.25, shuffle=True)\n"
      ],
      "metadata": {
        "id": "yIr9X-xHno2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u_model.fit(X_train, Y_train, batch_size=1, epochs=75, validation_split=0.25, shuffle=True)\n"
      ],
      "metadata": {
        "id": "pMV2zLRZnrR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predict using the model\n",
        "Y_hat = model.predict(X_test)\n",
        "\n",
        "# Calculate the error for each feature\n",
        "error = np.abs((Y_hat - Y_test) / (Y_test + 1e-8)) # Add a small constant to avoid division by zero\n",
        "\n",
        "# Plot the error for the first feature\n",
        "plt.plot(error[error > 3])\n",
        "plt.title('Error for Feature 1')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Error')\n",
        "plt.show()\n",
        "\n",
        "# Plot the error for the second feature\n",
        "plt.plot(error[error > 5])\n",
        "plt.title('Error for Feature 2')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "svsV1YFHnrvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import Booster\n",
        "\n",
        "model._Booster.save_model('model.bin')\n",
        "\n",
        "def load_xgb_model():\n",
        "    _m = XGBClassifier()\n",
        "    _b = Booster()\n",
        "    _b.load_model('model.bin')\n",
        "    _m._Booster = _b\n",
        "    return _m\n",
        "\n",
        "model = load_xgb_model()"
      ],
      "metadata": {
        "id": "m6MXTceanxJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "u_model.save('u_model.hd5')\n",
        "u_model = load_model('u_model.hd5')"
      ],
      "metadata": {
        "id": "0W6rcbaen1Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'X' contains your features and 'y' contains your labels\n",
        "# Assuming 'X' contains your features and 'y' contains your labels\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "utO8X9uF7eBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "RRZEeCXz7tGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "# Create a confusion matrix display\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Parkinson', 'Parkinson'])\n",
        "\n",
        "# Plot confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for Parkinson\\'s Disease')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gn9g-FUi7xTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "LB27ww6uot5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to generate spiral data with added noise to simulate Parkinson's tremors\n",
        "def generate_spiral(a, b, num_points, noise_level):\n",
        "    # Generate a set of angles from 0 to 4*pi\n",
        "    t = np.linspace(0, 4*np.pi, num_points)\n",
        "\n",
        "    # Parametric equations of a spiral with added noise\n",
        "    x = (a + b * t) * np.cos(t) + np.random.normal(0, noise_level, num_points)\n",
        "    y = (a + b * t) * np.sin(t) + np.random.normal(0, noise_level, num_points)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Set parameters for the spiral\n",
        "a = 1     # Constant that controls the overall size of the spiral\n",
        "b = 0.2   # Constant that controls how tightly the spiral coils\n",
        "num_points = 1000  # Number of points to generate for the spiral\n",
        "noise_level = 0.1  # Level of noise to add to simulate Parkinson's tremors\n",
        "\n",
        "# Generate the spiral data with added noise\n",
        "x, y = generate_spiral(a, b, num_points, noise_level)\n",
        "\n",
        "# Plot the generated spiral\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(x, y, 'b-', linewidth=2)  # Plotting the spiral with blue color and solid line\n",
        "plt.title('Parkinson\\'s Disease Spiral Drawing')  # Set the title of the plot\n",
        "plt.xlabel('X')  # Label for the x-axis\n",
        "plt.ylabel('Y')  # Label for the y-axis\n",
        "plt.grid(True)   # Enable gridlines on the plot\n",
        "plt.show()       # Display the plot\n"
      ],
      "metadata": {
        "id": "VC-hoSE2BYHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create data for plotting\n",
        "x = np.linspace(-2, 2, 400)\n",
        "y = np.linspace(-2, 2, 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
        "\n",
        "# Create the figure and axis objects\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Plot the surface\n",
        "ax.contourf(X, Y, Z, cmap='viridis')\n",
        "\n",
        "# Draw some shapes\n",
        "circle = plt.Circle((0, 0), 1, color='red', alpha=0.5)\n",
        "ax.add_artist(circle)\n",
        "\n",
        "rectangle = plt.Rectangle((-1, -1), 2, 2, color='blue', alpha=0.5)\n",
        "ax.add_artist(rectangle)\n",
        "\n",
        "# Set plot title and labels\n",
        "ax.set_title('Artistic Plot')\n",
        "ax.set_xlabel('X-axis')\n",
        "ax.set_ylabel('Y-axis')\n",
        "\n",
        "# Show the plot\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bS9t0HkDDqE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U scikit-learn\n",
        "import sklearn\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "_Cmt0FCFds31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/archive/parkinsons.data')\n",
        "print(data.columns)"
      ],
      "metadata": {
        "id": "tOkQTHOSfQLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop('status', axis=1)\n",
        "Y = data['status']"
      ],
      "metadata": {
        "id": "nQdtRVuOff_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vSkHeZCAd8nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load your dataset (replace 'parkinson_data.csv' with your dataset filename)\n",
        "data = pd.read_csv('/content/drive/MyDrive/archive/parkinsons.data')\n",
        "\n",
        "# Selecting features and target variable\n",
        "X = data.drop(['status', 'name'], axis=1)  # Features (excluding 'name' and 'status' columns)\n",
        "Y = data['status']  # Target variable\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Training the XGBoost classifier\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Predictions\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating evaluation metrics\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "precision = precision_score(Y_test, Y_pred)\n",
        "recall = recall_score(Y_test, Y_pred)\n",
        "f1 = f1_score(Y_test, Y_pred)\n",
        "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "gm = np.sqrt(sensitivity * specificity)\n",
        "\n",
        "# Printing the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Geometric Mean:\", gm)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "id": "JDdp4oOFsG08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/archive/parkinsons.data')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = data.drop(['name', 'status'], axis=1)\n",
        "Y = data['status']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the training and testing sets to numpy arrays\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "\n",
        "# Train the SVM classifier\n",
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "4sm-B-0RgNsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "9hiHAJbMghVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Precision:\", precision_score(Y_test, Y_pred))\n",
        "print(\"Recall:\", recall_score(Y_test, Y_pred))\n",
        "print(\"F1-Score:\", f1_score(Y_test, Y_pred))"
      ],
      "metadata": {
        "id": "91zxsAYDeQKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pridiction using RandomForest\n"
      ],
      "metadata": {
        "id": "udOc5At4A0qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/archive/parkinsons.data')\n",
        "\n",
        "# Drop the 'name' column\n",
        "X = data.drop('name', axis=1)\n",
        "\n",
        "# Split the data into features and target\n",
        "Y = X.pop('status')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "Y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
        "print(\"Precision:\", precision_score(Y_test, Y_pred))\n",
        "print(\"Recall:\", recall_score(Y_test, Y_pred))\n",
        "print(\"F1-Score:\", f1_score(Y_test, Y_pred))"
      ],
      "metadata": {
        "id": "WblrSPDihPP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create and train your classification model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "3SeA84fzSgw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with the path to your dataset)\n",
        "data = pd.read_csv('/content/drive/MyDrive/archive/parkinsons.data')\n",
        "\n",
        "# Assuming 'status' is the target column, drop it to get features\n",
        "X = data.drop('status', axis=1)\n",
        "\n",
        "# Encode categorical variables using one-hot encoding\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Target variable\n",
        "y = data['status']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "fxeYabP_RY_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}